\chapter{Background}
\label{chapter:background}

\section{Distributed locking}

A Distributed System is defined by \citet[\p{2}]{coulouris2005distributed} as 
hardware or software components of networked computers performing activities by
communicating with each other only by passing messages. Said definition gives
it attributes such as concurrency%
\sidenote[3]{
  \emph{Concurrency}: Simultaneous execution of processes (may or may not be 
  in parallel).
}
, no global clock%
\sidenote[6]{
  \emph{No global clock}: The computers in the distributed system are not 
  co-ordinated using a single global clock and use other mechanisms such as
  vector clocks \citep{Lamclocks} for it.
}
and independent failures
\sidenote[11]{
  \emph{Independent failures}: Failure of individual computers does not lead
  to the failure of the entire system but, rather has other consequences such
  as degraded performance.
}.

One of the ways to co-ordinate concurrent processes in a loosely coupled 
distributed system is to use a distributed lock manager. It helps such
processes to synchronize their activities or to serialize their access to
shared resources.

A distributed lock manager can be implemented in many different ways. One such
method is using a consensus algorithm.

\section{Consensus algorithm}

\citet{Lamclocks} first suggested that distributed systems can be modelled as
state machines. Progress can be made by transitioning to different states based
on events generated by passing messages between these networked machines.

Consensus is arriving at a single decision by the agreement of participants in a
group. Consensus algorithms allows a group of connected processes agree with 
each other, which is important in case of failures. Solving consensus allows
us to use it as a primitive to solve more advanced problem in distributed
computing such as atomic commits and total order broadcasts. This is a problem
that is related to lot of other agreement problems \citep{GS01}.

In an asynchronous network, consensus is easy when there are no faults but, 
gets tricky otherwise \citep{Lampson:1996:HBH}. Further more, in such a network,
no algorithmexists that can reach consensus in case of even one faulty process
\citep{FisLynPat85}.

Paxos is one of many available consensus algorithms but, its core is the best
known asynchronous consensus algorithm \citep{Lampson:1996:HBH}. It is covered
in more detail in [[]].

\section{CAP Theorem}

The CAP theorem or Brewer's conjecture states that it is not 
possible to achieve consistency%
\sidenote{
  \emph{Consistency}: Requests sent to a distributed system is said to be
  consistent if the result of the request is the same as compared to sending
  the the request to a single node executing the request one by one.
}
, availability%
\sidenote[6]{
  \emph{Available}: Every request sent to the system must eventually terminate.
}
and partition tolerance%
\sidenote[9]{
  \emph{Partition tolerance}: Communication loss between set of nodes in the
  network.
}
at the same time in an asynchronous network model
\citep{journals/sigact/GilbertL02}. A choice of two of these attributes must
be made when designing a system in such a network.

In this project, we focus mainly on consistency and partition tolerance 
as a primary goal with availability as a secondary goal.

\section{Challenges}

Overcoming the inherent problems in a distributed system provides its own set of
challenges.

\subsection{Scale}

Scaling in this context refers to increase in throughput by increasing the
number of machines in the network. However, in the case of a distributed 
consensus based locking system, the throughtput is inversly proportial to the 
number of nodes in the network since it involves more steps and messages needed 
for agreement.

\subsection{Configuration}

Distributed systems need to plan ahead in terms of handling node failures and
should provide ways to replace failed nodes. 

The system should support dynamic configuration to allow increasing and
decreasing the cluster size as required.

\subsection{Failure tolerance}

There are many possible reasons causing failures in a distributed setup. 
Machines occasionally fail, messages can be lost in transit, networks can be
partitioned, disks can fail and so on.

One way to classify such failures is Byzantine%
\sidenote{
  A faulty component sends conflicting messages to different parts of the
  system.
}
and non-byzantine%
\sidenote[3]{
  A component either sends or doesn't send the message.
}
depending on its origin.

The software should be robust enough to tolerate such failures. This project 
only deals with non-byzantine failures.

\subsection{Finding and Fixing Bugs}

Many things can go wrong in a distributed system 
\citep{Rotem-gal-oz_fallaciesof}. The algorithms can be hard to implement, 
minute logical errors in the implementation could cause race conditions and
it is hard to estimate time and message complexities in advance.

This makes it hard to discover bugs, reproduce them, find its source and fix
them.

\subsection{Testing}

Testing distributed systems is a hard problem \citep{BoyCPW2003}.
Different types of tests such as unit tests%
\sidenote{
  Testing individual "units" of code.
}
, integration tests%
\sidenote[2]{
  Testing that different components of the system work together.
}
, system tests%
\sidenote[4]{
  Testing the system as a while works well and meets specified requirements.
}
, load tests%
\sidenote[6]{
  Amount of traffic the system can safely handle.
}
are needed for a robust implementation.

Furthermore, the implementation needs to be tested with different cluster sizes. 

